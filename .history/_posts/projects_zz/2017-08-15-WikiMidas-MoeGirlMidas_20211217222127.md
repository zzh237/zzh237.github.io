---
layout: post
title: WikiMidas and MoeGirlMidas
excerpt: ""
modified: 2017-08-15T14:17:25-04:00
categories: project
tags: [Reinforcement Learning]
image:
  teaser: /images/projects/2017-08-15-WikiMidas-MoeGirlMidas/moegirl_teaser.png
comments: true
share: true
---

### Introduction

I implemented Deep Q-networks using tensorflow to solve the LunarLander problem in the OpenGym AI. Three different techniques were applied to improve the performance of deep Q-Networks, which are <a href ="https://arxiv.org/pdf/1509.06461.pdf">double deep Q-Networks</a>, 
<a href ="https://arxiv.org/pdf/1511.06581.pdf">dueling deep Q-Networks</a>, and <a href ="https://arxiv.org/pdf/1511.05952.pdf">prioritized experience replay</a>. The scores per episode were compared [<a href ="https://drive.google.com/open?id=1sDkJUoM2ZCd9DocFZo8siLoN_U7iI32J">PDF</a>].   

<br />


### Results

<figure>
  <img src="/images/projects/lunarlander/algo_compare_per_ep.png" width="304" height="180">
</figure>
<figure>
  <img src="/images/projects/lunarlander/episode_0.gif" width="304" height="180">  
  <figcaption> Episode 0 </figcaption>
</figure>
<figure>
  <img src="/images/projects/lunarlander/episode_3000.gif" width="304" height="180">  
  <figcaption> Episode 5000 </figcaption>
</figure>



<!-- <div class = "titled-image">
<figure>
  <img src="{{ site.url }}/images/projects/2017-08-15-WikiMidas-MoeGirlMidas/midas.gif"/>
</figure>
</div> -->


