---
layout: post
title: "Probability, Coding and Information Theory"
excerpt: "Understand Negative Log Likelihood"
modified: 2018-02-03T14:17:25-04:00
categories: blog
tags: [Probability, Information Theory, Coding]
comments: true
share: true
---

To be continued.

### Introduction

Bit Per Character (BPC), Bit Per Word, Perplexity, Information Gain, Coding in Bits.

<br />

"The results are presented with two equivalent metrics: bits-per-character (BPC), which is the average value of $−\log_2 \text{Pr}(x_{t+1}|y_t)$ over the whole test set;
and perplexity which is two to the power of the average number of bits per word (the average word length on the test set is about 5.6 characters, so perplexity ≈
$2^{5.6\text{BPC}}$). Perplexity is the usual performance measure for language modelling. " [[Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850)]

The difference between discrete entropy and continuous entropy.
http://thirdorderscientist.org/homoclinic-orbit/2013/5/8/bridging-discrete-and-differential-entropy

Prefix codes and Binary tree

https://www.youtube.com/watch?v=bWSHmC1eSyY