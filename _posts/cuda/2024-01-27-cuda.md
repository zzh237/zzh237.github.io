---
layout: post
title: "Summary of Optimization"
category: cuda
tags: [cuda]
modified: 1-26-2024
comments: false
---


## Summary
I have renamed the file `cuda_prog.cu` to `baseline.cu`. Additionally, I have developed two incremental improvements, which are implemented in the files `improvement1.cu` and `improvement2.cu`. Furthermore, there is a hyperparameter tuning file, `improvement2_kernel_tune.cu`, specifically for the second improvement. 

The execution times for the baseline, improvement1, and the tuned version of improvement2 are 32 ms, 7.65 ms, and 1.83 ms, respectively.

## Run 
1. **Compile Code**
   - Command: `make TARGET=improvement1`  
     (This will remove the existing `improvement1.x` and create a new `improvement1.x`.)
   - Command: `make TARGET=improvement2`  
     (This will remove the existing `improvement2.x` and create a new `improvement2.x`.)
   - Command: `make TARGET=baseline`  
     (This will remove the existing `baseline.x` and create a new `baseline.x`.)

2. **Execution**
   - Command: `./improvement1.x`  
     (This will display the runtime of `improvement1`, which is approximately 7.65 ms.)
   - Command: `./improvement2.x`  
     (This will display the runtime of `improvement2`, which is approximately 1.85 ms.)
   - Command: `./baseline.x`  
     (This will display the runtime of `baseline`, which is approximately 32 ms.)

3. **One-Time Execution**
   - Command: `./run_all.sh`  
     (This will execute all of the above steps that compiling, execution, saving results at once.)


## Improvement1 (7.65 ms vs  32.46 ms of baseline)
1. The matrix is stored in row-major order. I allow each thread in a warp to load one matrix values simultaneously. Computations are then performed, and the results are written back directly to global memory, since no reuse data, the usage of shared memory is not considered. The global memory access is coalesced by group of threads.
2. I set up each block contains a maximum of 256 threads (blockDim.x = 256, blockDim.y = 1). This implies that there are 256 threads in the x-direction and 1 in the y-direction. The choice of 256 threads is significant because it is a multiple of 32, ensuring alignment with the number of threads per warp.  The `blockDim.x` values can be chosen from the set {32, 64, 128, 256, 512, 1024}, with common choices for mainstream architectures being {128, 256, 512}. In my case, I am using a GTX 2080Ti GPU.
2. Then I configured a grid with 32 blocks along the x-axis and 8 * 1024 blocks along the y-axis (gridDim.x = dimx/blockDim.x, gridDim.y = dimy). This configuration results in a total of 32 * 8 * 1024 blocks. This setup is effectively designed so that, along the x-axis of the grid, it processes one row of the matrix. 

```cpp
__global__ void kernel_A(float *g_data, int dimx, int dimy, int niterations) {
    
    // Calculate row and column for float elements
    int rowid = blockIdx.y * blockDim.y + threadIdx.y;
    int colid = blockIdx.x * blockDim.x + threadIdx.x;

    // Calculate linear index for float elements
    int ix = rowid * dimx + colid;

    // Ensure we don't go out of bounds
    if (rowid < dimy && colid < dimx) {
        float value = g_data[ix];
        // Perform operations on each component of float
        for (int i = 0; i < niterations; i++) {
          switch(colid % 4) {
                case 0: value += sqrtf(logf(value) + 1.f); break;
                case 1: value += sqrtf(cosf(value) + 1.f); break;
                case 2: value += sqrtf(sinf(value) + 1.f); break;
                case 3: value += sqrtf(tanf(value) + 1.f); break;
            }
        }
        // Write back results
        g_data[ix] = value;
    }
}

void launchKernel(float * d_data, int dimx, int dimy, int niterations) {
  // Only change the contents of this function and the kernel(s). You may
  // change the kernel's function signature as you see fit. 
  int threads_x = 256;
  dim3 block(threads_x, 1);
  dim3 grid(dimx/threads_x, dimy);
  kernel_A<<<grid, block>>>(d_data, dimx, dimy, niterations);
}
```

## Improvement2, final used (1.83 ms vs  32.46 ms of baseline)
1. **Control Divergence Issue**: There was a potential control divergence issue, as threads in a warp could take different paths due to the if-else statement. By using `float4`, this divergence is reduced.
2. **Memory Access Optimization**: I cast the pointer to `float4*`, allowing each thread to load four matrix values simultaneously. Computations are then performed, and the results are written back directly to global memory. This method enhances memory access efficiency.
3. **Efficient Kernel Processing**: Operations within the kernel are performed on the data in `float4` chunks. This approach leads to more efficient processing due to reduced control divergence and optimized memory access.

```cpp
__global__ void kernel_A(float *g_data, int dimx, int dimy, int niterations) {
    // Cast the input pointer to float4*
    float4* A2 = (float4*)g_data;

    // Calculate row and column for float4 elements
    int rowid = blockIdx.y * blockDim.y + threadIdx.y;
    int colid = blockIdx.x * blockDim.x + threadIdx.x;

    // Calculate linear index for float4 elements
    int ix = rowid * (dimx / 4) + colid;

    // Ensure we don't go out of bounds
    if (rowid < dimy && colid < dimx / 4) {
        float4 value = A2[ix];
        // Perform operations on each component of float4
        for (int i = 0; i < niterations; i++) {
          int colidx = colid * 4; // Calculate the actual column index
          value.x += (colidx % 4 == 0) ? sqrtf(logf(value.x) + 1.f) : (colidx % 4 == 1) ? sqrtf(cosf(value.x) + 1.f) : (colidx % 4 == 2) ? sqrtf(sinf(value.x) + 1.f) : sqrtf(tanf(value.x) + 1.f);
          value.y += ((colidx + 1) % 4 == 0) ? sqrtf(logf(value.y) + 1.f) : ((colidx + 1) % 4 == 1) ? sqrtf(cosf(value.y) + 1.f) : ((colidx + 1) % 4 == 2) ? sqrtf(sinf(value.y) + 1.f) : sqrtf(tanf(value.y) + 1.f);
          value.z += ((colidx + 2) % 4 == 0) ? sqrtf(logf(value.z) + 1.f) : ((colidx + 2) % 4 == 1) ? sqrtf(cosf(value.z) + 1.f) : ((colidx + 2) % 4 == 2) ? sqrtf(sinf(value.z) + 1.f) : sqrtf(tanf(value.z) + 1.f);
          value.w += ((colidx + 3) % 4 == 0) ? sqrtf(logf(value.w) + 1.f) : ((colidx + 3) % 4 == 1) ? sqrtf(cosf(value.w) + 1.f) : ((colidx + 3) % 4 == 2) ? sqrtf(sinf(value.w) + 1.f) : sqrtf(tanf(value.w) + 1.f);
        }
        // Write back results
        A2[ix] = value;
    }
}

void launchKernel(float * d_data, int dimx, int dimy, int niterations) {
  // Only change the contents of this function and the kernel(s). You may
  // change the kernel's function signature as you see fit. 
  int threads_x = 256
  dim3 block(threads_x, 1);
  dim3 grid(dimx/(threads_x*4), dimy);
  kernel_A<<<grid, block>>>(d_data, dimx, dimy, niterations);
}
```

**Configuration of grid and block dimensions**
1. Each block in my configuration contains a maximum of 256 threads. This implies that there are 256 threads in the x-direction and 1 in the y-direction. The choice of 256 threads is significant because it is a multiple of 32, ensuring alignment with the number of threads per warp.
2. Then I configured a grid with 8 blocks along the x-axis and 8 * 1024 blocks along the y-axis (`gridDim.x = 8`, `gridDim.y = dimy`). This configuration results in a total of 8 * 8 * 1024 blocks. Along the x-coordinate of the grid, these 8 blocks collectively manage 8 * 256 threads, aligning with the processing of 4 adjacent operations for each thread. This setup is effectively designed so that, along the x-coordinate of the grid, it processes one row of the matrix. 

## Supplementary: Hyperparameter-Tuning for Improvement2
**Optimization of Grid and Block Dimensions**
1. The last experiment I undertook was to test different block and grid dimensions. I experimented with `blockDim.x` values chosen from {32, 64, 128, 256, 512, 1024}, and `gridDim.x` values chosen from {64, 32, 16, 8, 4, 2}. The times for each of them are {2.58, 1.87, 1.88, 1.83, 1.84, 1.94} ms. Thus, the potentially optimal configuration I found was with `blockDim.x` equal to 256 and `gridDim.x` equal to 8. Furthermore, based on the criterion that the block size of a kernel should consider high occupancy on the Streaming Multiprocessors (SMs), the block size should approximate the maximum number of threads an SM can support. Otherwise, achieving 100% occupancy may not be feasible. Conventionally, the mainstream architecture GPUs have a maximum thread capacity of 512 per SM. Additionally, considering the register allocation, each SM can typically support at least 128 or 256 threads.
2. Command: `./run_improvement_kernel_tuning.sh`  
   (This will execute the above experiment and save the results to the file `improvement_kernel_tuning_result.txt`.)

## Improvement3: Not Better than Improvement2 (1.88 ms vs 32.46 ms of Baseline)
1. I defined a shared memory array to store a portion of `g_data` that threads in a block will access. Each thread in a block loads data from global memory to shared memory. Computations are performed on the data in shared memory. The results are then written back from shared memory to global memory. This approach (code is in `improvement3.cu`) did not yield better or worse results compared to the global memory approach since there's no reuse of data loaded into shared memory by multiple threads.


## Improvement4: Not Better Than Improvement2 (2.64ms vs. 32.46ms of Baseline)

**Configuration of Grid and Block Dimensions Considering SMs**
1. With the block dimensions established, I set `gridDim.x = 8` and `gridDim.y = dimy`, ensuring that each thread processes at least one element. Given that the GPU can schedule a number of blocks at one time, which is equal to the number of Streaming Multiprocessors (SMs, `num_sms=68`) times the maximum number of blocks per SM (8), all SMs should ideally complete the computation of these blocks almost simultaneously before processing the next batch. Each batch is referred to as a wave. Therefore, I adjusted the grid dimensions in the `launchKernel` function and added a loop for multiple kernel launches (code is in `improvement4.cu`). This strategy divides the work among multiple kernel launches based on the number of SMs and the maximum number of blocks each SM can handle. However, this approach did not yield better performance than my Improvement2. One potential reason for the observed performance outcome is that the overhead associated with launching a new block on the GPU is quite small. Therefore, having a high count of blocks should not be a major concern. 


```cpp
void launchKernel(float *d_data, int dimx, int dimy, int niterations) {
    int threads_x = 256;
    dim3 block(threads_x, 1);

    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, 0);
    int num_sms = prop.multiProcessorCount;
    int sm_blocks = num_sms * 8;  // Total number of blocks across all SMs

    // Calculate the number of blocks in the grid's x dimension
    int blocks_x = dimx / (4 * threads_x);  // Note the division by 4 due to float4
    // Calculate the maximum number of rows processed per kernel launch
    int maxRowsPerLaunch = (sm_blocks / blocks_x);

    // Launch the kernel multiple times to cover all rows
    for (int startRow = 0; startRow < dimy; startRow += maxRowsPerLaunch) {
        int rowsToProcess = min(maxRowsPerLaunch, dimy - startRow);
        dim3 grid(blocks_x, rowsToProcess);
        kernel_A<<<grid, block>>>((float4*)d_data, dimx, rowsToProcess, niterations, startRow);
    }
}
```
